{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for a kaggle competition: https://www.kaggle.com/c/bosch-production-line-performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part that produces default_xgboost_results.csv has the highest accuracy right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import pandas\n",
    "import time\n",
    "import io\n",
    "import zipfile\n",
    "import numpy\n",
    "print('imports complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FYI: This takes about a minute and a half\n",
    "def load_and_unzip(filename):\n",
    "    print('loading and unzipping large training data file %s' % (filename))\n",
    "    t0 = time.time()\n",
    "    df = pandas.read_csv(filename, compression='zip')\n",
    "    t1 = time.time()\n",
    "    print('completed in %.3f seconds' % (t1-t0))\n",
    "    return df\n",
    "\n",
    "#train_numeric_filename = '../resources/train_numeric.csv.zip'\n",
    "#df_explore = load_and_unzip(train_numeric_filename)\n",
    "#test_numeric_filename = '../resources/test_numeric.csv.zip'\n",
    "#df_test = load_and_unzip(test_numeric_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the empty data with 0. get the mean and std of the entire training dataset, minus the id colum and the dependent variable column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_explore = pandas.read_csv('../resources/split_train_numeric_a.csv')\n",
    "df_explore.fillna(0,inplace=True)\n",
    "df_explore_meta = df_explore[df_explore.keys()[1:-1]].describe()\n",
    "df_explore_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_training_DMatrix(filename):\n",
    "    arr = numpy.genfromtxt(filename, delimiter=',', skip_header=True, filling_values=0)\n",
    "    return xgboost.DMatrix(arr[:,1:-1], label=arr[:,-1])\n",
    "# load the testing file, without labels\n",
    "def load_testing_DMatrix(filename):\n",
    "    arr = numpy.genfromtxt(filename, delimiter=',', skip_header=True, filling_values=0)\n",
    "    return arr[:,0], xgboost.DMatrix(arr[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtrain = load_training_DMatrix('../resources/train_numeric.csv')\n",
    "param = {'max_depth':2, 'eta':1, 'slient':1, 'objective':'binary:logistic'}\n",
    "num_round = 10\n",
    "bst = xgboost.train(param, dtrain, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids, dtest = load_testing_DMatrix('../resources/test_numeric.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('default_xgboost_results.csv', 'w') as f:\n",
    "    f.write('Id,Response\\n')\n",
    "    for i, p in enumerate(pred):\n",
    "        f.write('%d,%s\\n'%(ids[i],'1' if p > 0.5 else '0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the xgboost classifier\n",
    "cross validation with k fold\n",
    "grid search over a few parameters\n",
    "data stored in pandas DataFrame\n",
    "still only using just the numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import sklearn.grid_search\n",
    "import sklearn.cross_validation\n",
    "import xgboost.sklearn\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('../resources/train_numeric.csv')\n",
    "data.fillna(0,inplace=True)\n",
    "target_name = 'Response'\n",
    "target = data[target_name]\n",
    "features = data.drop(target_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perf_metric(y_true, y_predict):\n",
    "    return sklearn.metrics.matthews_corrcoef(y_true, y_predict)\n",
    "\n",
    "def fit_model(X,y):\n",
    "    cv_sets = sklearn.cross_validation.ShuffleSplit(X.shape[0], n_iter=10, test_size=0.1, random_state=0)\n",
    "    clf = xgboost.sklearn.XGBClassifier(nthread=14)\n",
    "    params = {\n",
    "                'max_depth' : [6],\n",
    "                'reg_lambda' : [1]\n",
    "             }\n",
    "    scoring_fnc = sklearn.metrics.make_scorer(perf_metric)\n",
    "    grid = sklearn.grid_search.GridSearchCV(clf, params, scoring=scoring_fnc, cv=cv_sets, verbose=2)\n",
    "    grid = grid.fit(X,y)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "clf = fit_model(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pandas.read_csv('../resources/test_numeric.csv')\n",
    "pred = clf.predict(test_data)\n",
    "with open('xgboost_all_numeric_results.csv', 'w') as f:\n",
    "    f.write('Id,Response\\n')\n",
    "    for i, p in enumerate(pred):\n",
    "        f.write('%d,%d\\n'%(test_data['Id'][i],p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent for linear regression with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "x_data = numpy.random.rand(100).astype(numpy.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "W = tensorflow.Variable(tensorflow.random_uniform([1], -1.0, 1.0))\n",
    "b = tensorflow.Variable(tensorflow.zeros([1]))\n",
    "y = W * x_data + b\n",
    "\n",
    "loss = tensorflow.reduce_mean(tensorflow.square(y-y_data))\n",
    "optimizer = tensorflow.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "init = tensorflow.initialize_all_variables()\n",
    "\n",
    "sess = tensorflow.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if (step % 20 == 0) :\n",
    "        print(step, sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try tensorflow and do a LSTM for anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.918\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.examples.tutorials.mnist\n",
    "mnist = tensorflow.examples.tutorials.mnist.input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tensorflow\n",
    "sess = tensorflow.InteractiveSession()\n",
    "\n",
    "x = tensorflow.placeholder(tensorflow.float32, shape=[None, 784])\n",
    "y_ = tensorflow.placeholder(tensorflow.float32, shape=[None, 10])\n",
    "\n",
    "W = tensorflow.Variable(tensorflow.zeros([784,10]))\n",
    "b = tensorflow.Variable(tensorflow.zeros([10]))\n",
    "\n",
    "sess.run(tensorflow.initialize_all_variables())\n",
    "\n",
    "y = tensorflow.nn.softmax(tensorflow.matmul(x, W) + b)\n",
    "\n",
    "cross_entropy = tensorflow.reduce_mean(-tensorflow.reduce_sum(y_ * tensorflow.log(y), reduction_indices=[1]))\n",
    "\n",
    "train_step = tensorflow.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x:batch[0], y_:batch[1]})\n",
    "\n",
    "correct_prediction = tensorflow.equal(tensorflow.argmax(y,1), tensorflow.argmax(y_,1))\n",
    "accuracy = tensorflow.reduce_mean(tensorflow.cast(correct_prediction, tensorflow.float32))\n",
    "print(accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 layer conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tensorflow.truncated_normal(shape, stddev=0.1)\n",
    "    return tensorflow.Variable(initial)\n",
    "def bias_variable(shape):\n",
    "    initial = tensorflow.constant(0.1, shape=shape)\n",
    "    return tensorflow.Variable(initial)\n",
    "def conv2d(x, W):\n",
    "    return tensorflow.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tensorflow.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_image = tensorflow.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tensorflow.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tensorflow.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tensorflow.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tensorflow.nn.relu(tensorflow.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tensorflow.placeholder(tensorflow.float32)\n",
    "h_fc1_drop = tensorflow.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024,10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tensorflow.nn.softmax(tensorflow.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "cross_entropy = tensorflow.reduce_mean(-tensorflow.reduce_sum(y_ * tensorflow.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tensorflow.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tensorflow.equal(tensorflow.argmax(y_conv,1), tensorflow.argmax(y_,1))\n",
    "accuracy = tensorflow.reduce_mean(tensorflow.cast(correct_prediction, tensorflow.float32))\n",
    "sess.run(tensorflow.initialize_all_variables())\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
